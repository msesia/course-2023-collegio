{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom data generation models from the mfpi folder\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from mfpi import data_gen_models as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generating model\n",
    "\n",
    "We generate data from a toy model with 10 explanatory variables and a qualitative label for each sample, which is designed to mimic \"heteroschedasticity\" in a classification setting.\n",
    "More precisely, the first variable controls the \"noise level\" in the label: small values of $X_0$ mean that all labels are more or less equally likely; large values of $X_0$ mean that one label is much more likely than the others (which one is determined by the other features).\n",
    "To clarify, we visualize below the distribution of the true label probabilities (for one value of the label) as a function of $X_0$, which here can take only two possible values for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(2023)\n",
    "\n",
    "# Pre-defined model\n",
    "p = 10                                                 # Number of features\n",
    "K = 10                                                 # Number of possible labels\n",
    "data_model = data.Model_Class1(K,p)                    # Data generating model\n",
    "\n",
    "# Sample data\n",
    "n = 500                                                # Number of data samples\n",
    "X_data = data_model.sample_X(n)                        # Generate the data features\n",
    "Y_data = data_model.sample_Y(X_data)                   # Generate the data labels conditional on the features\n",
    "\n",
    "# Sample test data\n",
    "n_test = 1000                                          # Number of test samples\n",
    "X_test = data_model.sample_X(n_test)                   # Generate independent test data\n",
    "Y_test = data_model.sample_Y(X_test)\n",
    "\n",
    "# Plot the marginal distribution of the class labels\n",
    "plt.hist(Y_data, bins=K-1, rwidth=0.5, align=\"left\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Marginal distribution of class labels\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the conditional distribution of the class labels\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "idx_0 = np.where(X_data[:,0]==1)[0]\n",
    "axs[0].hist(Y_data[idx_0], bins=K-1, rwidth=0.5, align=\"left\")\n",
    "axs[0].set_xlabel(\"Label\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "axs[0].set_title(\"P(Y | X0 = 1)\")\n",
    "idx_0 = np.where(X_data[:,0]==-8)[0]\n",
    "axs[1].hist(Y_data[idx_0], bins=K-1, rwidth=0.5, align=\"left\")\n",
    "axs[1].set_xlabel(\"Label\")\n",
    "axs[1].set_title(\"P(Y | X0 == -8)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle classifier\n",
    "\n",
    "Let's apply the classification oracle we learnt about during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mfpi import classification\n",
    "\n",
    "def oracle(pi, alpha, randomize=True):\n",
    "    prediction_rule = classification.ProbabilityAccumulator(pi)\n",
    "    S = prediction_rule.predict_sets(alpha, randomize=randomize)\n",
    "    return S\n",
    "\n",
    "# Compute true class probabilities for every sample\n",
    "pi = data_model.compute_prob(X_test)\n",
    "\n",
    "# Nominal coverage: 1-alpha \n",
    "alpha = 0.1\n",
    "\n",
    "# Oracle prediction sets\n",
    "S_oracle = oracle(pi, alpha)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to evaluate the quality of the oracle predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(S, X, Y, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate performance metrics for a set of classification predictions\n",
    "    Computes:\n",
    "    - marginal coverage\n",
    "    - unbiased estimate of worst-slab coverage\n",
    "    - average size of sets\n",
    "    - average size of sets that contain the true Y\n",
    "    \n",
    "    Input\n",
    "    S         : n - long list of prediction sets (each set is a discrete array)\n",
    "    X         : n x p data matrix of explanatory variables\n",
    "    Y         : n x 1 vector of response variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estimate worst-slab coverage\n",
    "    wsc_coverage = classification.wsc_unbiased(X, Y, S)\n",
    "    \n",
    "    # Number of samples\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Output placeholder\n",
    "    marginal_coverage = None\n",
    "    size = None\n",
    "    size_cover = None\n",
    "    \n",
    "    # Compute marginal coverage\n",
    "    marginal_coverage = np.mean([Y[i] in S[i] for i in range(n)])\n",
    "    \n",
    "    # Compute average size of prediction sets\n",
    "    size = np.mean([len(S[i]) for i in range(n)])\n",
    "    \n",
    "    # Compute average size of prediction sets that contain the true label\n",
    "    idx_cover = np.where([Y[i] in S[i] for i in range(n)])[0]\n",
    "    size_cover = np.mean([len(S[i]) for i in idx_cover])\n",
    "    \n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print('Marginal coverage       : {:2.3%}'.format(marginal_coverage))\n",
    "        print('WS conditional coverage : {:2.3%}'.format(wsc_coverage))\n",
    "        print('Average size            : {:2.3f}'.format(size))\n",
    "        print('Average size | cover    : {:2.3f}'.format(size_cover))\n",
    "        \n",
    "    return marginal_coverage, wsc_coverage, size, size_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_predictions(S_oracle, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-box classifier\n",
    "\n",
    "We will use a multi-layer Perceptron classifier.\n",
    "This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Fit the black-box classifier\n",
    "black_box = MLPClassifier(learning_rate_init = 0.01, max_iter = 1000, hidden_layer_sizes = 100, \n",
    "                          random_state = 2023)\n",
    "\n",
    "black_box.fit(X_data, Y_data)\n",
    "\n",
    "# Make point predictions for test data\n",
    "Y_hat = black_box.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract conditional class probability estimates from this black-box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate class probabilities for test points \n",
    "pi_hat = black_box.predict_proba(X_test)\n",
    "pi_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the estimated class probabilities to the true class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract estimated probabilities for observed class\n",
    "prob_est = np.array([pi_hat[i,Y_test[i]] for i in range(n_test)])\n",
    "prob_est\n",
    "\n",
    "# Compute true conditional probabilities from data generating model\n",
    "prob_true = data_model.compute_prob(X_test)\n",
    "prob_true = np.array([prob_true[i,Y_test[i]] for i in range(n_test)])\n",
    "\n",
    "# Make scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(prob_true, prob_est, alpha=0.1)\n",
    "ax.set_xlabel(\"True probability of Y | X\")\n",
    "ax.set_ylabel(\"Estimated probability of Y | X\")\n",
    "ax.set_xlim(-0.05,1.05)\n",
    "ax.set_ylim(-0.05,1.05)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the black-box classifier is over-confident.\n",
    "\n",
    "Let's see what happens if we plug these probability estimates into the oracle classification rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black-box plugin prediction sets\n",
    "S_plugin = None\n",
    "\"\"\"TODO: write your code here (1 line)\"\"\"\n",
    "\n",
    "# Evaluate prediction sets\n",
    "\"\"\"TODO: write your code here (1 line)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-conformal classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats.mstats import mquantiles\n",
    "\n",
    "def sc_classification(X, Y, X_test, alpha):\n",
    "    \"\"\"\n",
    "    Compute split-conformal classification prediction sets.\n",
    "    Uses neural network as a black box \n",
    "    \n",
    "    Input\n",
    "    X         : n x p data matrix of explanatory variables\n",
    "    Y         : n x 1 vector of response variables\n",
    "    X_test    : n x p test data matrix of explanatory variables\n",
    "    alpha     : 1 - target coverage level \n",
    "    \"\"\"\n",
    "    \n",
    "    # Output placeholder\n",
    "    S = None\n",
    "    \n",
    "    # Split the data into training and calibration sets\n",
    "    X_train, X_calib, Y_train, Y_calib = train_test_split(X, Y, test_size=0.5, random_state=2023)\n",
    "    \n",
    "    # Fit a quantile regression model\n",
    "    black_box = MLPClassifier(learning_rate_init = 0.01, max_iter = 1000, hidden_layer_sizes = 64, \n",
    "                              random_state = 2023)\n",
    "    black_box.fit(X_train, Y_train)\n",
    "\n",
    "    # Estimate class probabilities for calibration points\n",
    "    pi_hat = black_box.predict_proba(X_calib)\n",
    "\n",
    "    # Define prediction rule with plugin probability estimates\n",
    "    prediction_rule = classification.ProbabilityAccumulator(pi_hat)\n",
    "    \n",
    "    # Compute conformity scores\n",
    "    n_calib = len(Y_calib)\n",
    "    epsilon = np.random.uniform(low=0.0, high=1.0, size=n_calib)\n",
    "    scores = prediction_rule.calibrate_scores(Y_calib, epsilon = epsilon)\n",
    "    \n",
    "    # Compute suitable empirical quantile of absolute residuals\n",
    "    \"\"\"TODO: write your code here (2 lines)\"\"\"\n",
    "\n",
    "    # Construct prediction sets for test data\n",
    "    \"\"\"TODO: write your code here (2 lines)\"\"\"\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired coverage level (1-alpha)\n",
    "alpha = 0.1\n",
    "\n",
    "# Apply split conformal classification\n",
    "S = sc_classification(X_data, Y_data, X_test, alpha)\n",
    "\n",
    "# Evaluate prediction sets\n",
    "metrics = evaluate_predictions(S, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
